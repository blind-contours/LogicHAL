current_rule <- current_best_rules[[index]]
current_complexity <- count_logical_operators(current_rule)
new_complexity <- count_logical_operators(rule_desc)
if (new_complexity < current_complexity) {
current_best_rules[[index]] <- rule_desc
break
} else if (new_complexity == current_complexity) {
current_best_rules <- c(current_best_rules, rule_desc)
break
}
}
}
}
# Function to create a subtree
create_subtree <- function(score, rule, type) {
if (any(score != 0, na.rm = TRUE)) {
max_score_index <- which.max(score)
best_root_score <- score[max_score_index]
root_rule_desc <- rule[max_score_index]
if (best_root_score >= parent_score) {
# Evaluate the root rule using dplyr
root_rule <- data %>%
mutate(rule = eval(parse(text = root_rule_desc))) %>%
pull(rule)
# Update the best scores and best rule paths if this is the best so far
update_best_info(score = best_root_score, rule_desc = root_rule_desc)
# Recursive call for the subtree
subtree_result <- build_logic_tree(data = data, outcome = outcome, columns = columns,
max_depth = max_depth, current_depth = current_depth + 1,
parent_score = best_root_score, previous_rule = root_rule,
previous_rule_name = root_rule_desc, best_info_env = best_info_env, max_trees = max_trees)
return(list(
split = root_rule_desc,
score = round(best_root_score, 3),
subtree = subtree_result$tree
))
}
}
return(NULL)
}
score = score_indiv_ruleAnd
rule = Rule_indiv_And
type = "indiv_ruleAnd"
max_score_index <- which.max(score)
best_root_score <- score[max_score_index]
root_rule_desc <- rule[max_score_index]
root_rule_desc
parent_score
best_root_score >= parent_score
# Evaluate the root rule using dplyr
root_rule <- data %>%
mutate(rule = eval(parse(text = root_rule_desc))) %>%
pull(rule)
# Update the best scores and best rule paths if this is the best so far
update_best_info(score = best_root_score, rule_desc = root_rule_desc)
best_info_env$best_rule_paths
best_info_env$best_scores
load_all()
# Recursive call for the subtree
subtree_result <- build_logic_tree(data = data, outcome = outcome, columns = columns,
max_depth = max_depth, current_depth = current_depth + 1,
parent_score = best_root_score, previous_rule = root_rule,
previous_rule_name = root_rule_desc, best_info_env = best_info_env, max_trees = max_trees)
best_info_env$best_rule_paths
best_info_env$best_scores
# Build next best tree
best_info_env <- new.env()
best_info_env$best_scores <- rep(0, max_trees)
best_info_env$best_rule_paths <- list()
best_info_env$best_complexities <- rep(Inf, max_trees)
# Function to update the best scores and rule paths in best_info_env
update_best_info <- function(score, rule_desc) {
current_best_scores <- best_info_env$best_scores
current_best_rules <- best_info_env$best_rule_paths
current_best_complexities <- best_info_env$best_complexities
new_complexity <- count_logical_operators(rule_desc)
# Check if the new score is better than or equal to the minimum score in the current best scores
if (score >= min(current_best_scores)) {
if (score %in% current_best_scores) {
# If the score ties with any current best score, consider complexity
tied_indices <- which(current_best_scores == score)
for (index in tied_indices) {
current_rule <- current_best_rules[[index]]
current_complexity <- current_best_complexities[index]
if (new_complexity < current_complexity) {
current_best_rules[[index]] <- rule_desc
current_best_complexities[index] <- new_complexity
break
} else if (new_complexity == current_complexity) {
current_best_rules <- c(current_best_rules, rule_desc)
current_best_complexities <- c(current_best_complexities, new_complexity)
break
}
}
} else {
# If the new score is better than the minimum score, replace it
min_index <- which.min(current_best_scores)
current_best_scores[min_index] <- score
current_best_rules[[min_index]] <- rule_desc
current_best_complexities[min_index] <- new_complexity
}
# Sort based on scores and then by complexity
sorted_indices <- order(current_best_scores, -current_best_complexities, decreasing = TRUE)
best_info_env$best_scores <- current_best_scores[sorted_indices]
best_info_env$best_rule_paths <- current_best_rules[sorted_indices]
best_info_env$best_complexities <- current_best_complexities[sorted_indices]
}
}
load_all()
# Build next best tree
best_info_env <- new.env()
best_info_env$best_scores <- rep(0, max_trees)
best_info_env$best_rule_paths <- list()
best_info_env$best_complexities <- rep(Inf, max_trees)
result <- build_logic_tree(data = data, outcome = outcome, columns = columns, max_depth = max_depth, best_info_env = best_info_env, max_trees = max_trees)
trees <- result$best_rule_path
scores <- result$best_scores
trees
scores
load_all()
# Extract features from paths
path_features <- sapply(trees, function(path) as.numeric(eval(parse(text = path), envir = data)))
head(path_features)
path_features
colnames(path_features)
colnames(path_features) <- trees
head(path_features)
# Extract features from paths
path_features <- sapply(trees, function(path) as.numeric(eval(parse(text = path), envir = data)))
colnames(path_features) <- trees
# Fit Lasso with all combined features
lasso_model <- fit_lasso(all_features, data[[outcome]], family)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
family
family <- "binomial"
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
ceof(lasso_model)
coef(lasso_model)
load_all()
# Run LogicHAL
result <- LogicHAL(data = df, outcome = "outcome", columns = colnames(X), max_trees = 2, max_depth = 3)
# Run LogicHAL
result <- LogicHAL(data = df, outcome = "outcome", columns = colnames(X), max_trees = 2, max_depth = 3, family = "binomial")
# Print final model and trees used
coef(result$model)
# Number of observations
n <- 2000
# Number of features
p <- 100
# Create a binary matrix with some correlation
X <- matrix(rbinom(n * p, 1, 0.1), n, p)
# Create column names
colnames(X) <- paste0("var", 1:p)
# Create a continuous outcome based on a logic statement with added noise
Y <- 30 * (X[, 1] | X[, 8]) + 20 * (X[, 34]) + rnorm(n, mean = 0, sd = 0.1)
# Combine into a data frame
df <- data.frame(X, outcome = Y)
# Run LogicHAL
result <- LogicHAL(data = df, outcome = "outcome", columns = colnames(X), max_trees = 2, max_depth = 3, family = "gaussian")
data = df
outcome = "outcome"
columns = colnames(X)
max_trees = 2
max_depth = 3
family = "gaussian"
# Helper function to fit Lasso
fit_lasso <- function(features, outcome, family) {
model <- cv.glmnet(as.matrix(features), as.factor(outcome), family = family, alpha = 1)
return(model)
}
# Build next best tree
best_info_env <- new.env()
best_info_env$best_scores <- rep(0, max_trees)
best_info_env$best_rule_paths <- list()
best_info_env$best_complexities <- rep(Inf, max_trees)
result <- build_logic_tree(data = data, outcome = outcome, columns = columns, max_depth = max_depth, best_info_env = best_info_env, max_trees = max_trees)
trees <- result$best_rule_path
scores <- result$best_scores
trees
# Create a continuous outcome based on a logic statement with added noise
Y <- 30 * (X[, 1] | X[, 8]) + 20 * (X[, 34]) + rnorm(n, mean = 0, sd = 0.1)
# Combine into a data frame
df <- data.frame(X, outcome = Y)
data = df
outcome = "outcome"
columns = colnames(X)
max_trees = 2
max_depth = 3
family = "gaussian"
# Helper function to fit Lasso
fit_lasso <- function(features, outcome, family) {
model <- cv.glmnet(as.matrix(features), as.factor(outcome), family = family, alpha = 1)
return(model)
}
# Build next best tree
best_info_env <- new.env()
best_info_env$best_scores <- rep(0, max_trees)
best_info_env$best_rule_paths <- list()
best_info_env$best_complexities <- rep(Inf, max_trees)
result <- build_logic_tree(data = data, outcome = outcome, columns = columns, max_depth = max_depth, best_info_env = best_info_env, max_trees = max_trees)
trees <- result$best_rule_path
scores <- result$best_scores
trees
trees <- result$best_rule_path
scores <- result$best_scores
# Extract features from paths
path_features <- sapply(trees, function(path) as.numeric(eval(parse(text = path), envir = data)))
colnames(path_features) <- trees
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
path_features
head(path_features)
path_features[[1]]
path_features
path_features
View(path_features)
outcome
data[[outcome]]
# Build next best tree
best_info_env <- new.env()
best_info_env$best_scores <- rep(0, max_trees)
best_info_env$best_rule_paths <- list()
best_info_env$best_complexities <- rep(Inf, max_trees)
result <- build_logic_tree(data = data, outcome = outcome, columns = columns, max_depth = max_depth, best_info_env = best_info_env, max_trees = max_trees)
trees <- result$best_rule_path
scores <- result$best_scores
trees
# Extract features from paths
path_features <- sapply(trees, function(path) as.numeric(eval(parse(text = path), envir = data)))
colnames(path_features) <- trees
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
path_features
path_features <- as.data.frame(path_features)
path_features$`(var34 == 1) & ((var1 == 1) | (var8 == 1))`
table(path_features$`(var34 == 1) & ((var1 == 1) | (var8 == 1))`)
table(path_features$`(var1 == 1) & (var34 == 1)`)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
path_features <- path_features$`(var34 == 1) & ((var1 == 1) | (var8 == 1))`
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
# Extract features from paths
path_features <- sapply(trees, function(path) as.numeric(eval(parse(text = path), envir = data)))
path_features <- as.data.frame(path_features)
colnames(path_features) <- trees
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
data[[outcome]]
str(data[[outcome]])
# Fit Lasso with all combined features
lasso_model <- fit_lasso(as.matrix(path_features), data[[outcome]], family)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(as.matrix(path_features), as.matrix(data[[outcome]]), family)
summary(data[[outcome]])
is.na(data[[outcome]])
table(is.na(data[[outcome]]))
# Extract features from paths
path_features <- sapply(trees, function(path) as.numeric(eval(parse(text = path), envir = data)))
path_features
head(path_features)
path_features <- as.data.frame(path_features)
colnames(path_features) <- trees
head(path_features)
str(path_features)
path_features <- as.matrix(path_features)
colnames(path_features) <- trees
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[outcome], family)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(data, data[[outcome]], family)
head(data)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(as.matrix(data), data[[outcome]], family)
data[[outcome]]
family
family
# Helper function to fit Lasso
fit_lasso <- function(features, outcome, family) {
model <- cv.glmnet(as.matrix(features), as.factor(outcome), family = family, alpha = 1)
return(model)
}
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, as.numeric(data[[outcome]]), family)
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
?cv.glmnet
# Helper function to fit Lasso
fit_lasso <- function(features, outcome, family) {
model <- cv.glmnet(as.matrix(features), outcome, family = family, alpha = 1)
return(model)
}
# Fit Lasso with all combined features
lasso_model <- fit_lasso(path_features, data[[outcome]], family)
coef(lasso_model)
load_all()
# Number of observations
n <- 2000
# Number of features
p <- 100
# Create a binary matrix with some correlation
X <- matrix(rbinom(n * p, 1, 0.1), n, p)
# Create column names
colnames(X) <- paste0("var", 1:p)
# Create a continuous outcome based on a logic statement with added noise
Y <- 30 * (X[, 1] | X[, 8]) + 20 * (X[, 34]) + rnorm(n, mean = 0, sd = 0.1)
# Combine into a data frame
df <- data.frame(X, outcome = Y)
# Run LogicHAL
result <- LogicHAL(data = df, outcome = "outcome", columns = colnames(X), max_trees = 2, max_depth = 3, family = "gaussian")
# Print final model and trees used
coef(result$model)
load_all()
load_all()
# Helper function to check if a column is binary
is_binary_column <- function(column) {
unique_values <- unique(column)
return(length(unique_values) == 2 && all(unique_values %in% c(0, 1)))
}
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 2000
# Create binary indicators
binary_indicators <- matrix(rbinom(n * 20, 1, 0.5), n, 20)
colnames(binary_indicators) <- paste0("bin", 1:20)
# Create continuous variables
continuous_variables <- matrix(runif(n * 5, min = 0, max = 100), n, 5)
colnames(continuous_variables) <- paste0("cont", 1:5)
# Combine binary and continuous variables into a data frame
df <- data.frame(binary_indicators, continuous_variables)
# Create a continuous outcome based on a logic statement with added noise
# Example logic: outcome is high if bin1 == 1 and cont1 < 50
df$outcome <- 30 * (df$bin1 == 1 & df$cont1 < 50) + rnorm(n, mean = 0, sd = 1)
# Combine binary and continuous variables into a data frame
df <- data.frame(binary_indicators, continuous_variables)
columns <- colnames(df)[1:25]
columns
# Create a continuous outcome based on a logic statement with added noise
# Example logic: outcome is high if bin1 == 1 and cont1 < 50
df$outcome <- 30 * (df$bin1 == 1 & df$cont1 < 50) + rnorm(n, mean = 0, sd = 1)
data = df
df$outcome
outcome = "outcome"
columns = colnames(X)
max_trees = 2
max_depth = 3
family = "gaussian"
# Helper function to fit Lasso
fit_lasso <- function(features, outcome, family) {
model <- cv.glmnet(as.matrix(features), outcome, family = family, alpha = 1)
return(model)
}
# Helper function to check if a column is binary
is_binary_column <- function(column) {
unique_values <- unique(column)
return(length(unique_values) == 2 && all(unique_values %in% c(0, 1)))
}
# Helper function to check if a column is binary
is_binary_column <- function(column) {
unique_values <- unique(column)
return(length(unique_values) == 2 && all(unique_values %in% c(0, 1)))
}
source("~/LogicHAL/R/LogicHAL.R", echo=TRUE)
non_binary_columns <- columns[!sapply(data[columns], is_binary_column)]
data[columns]
columns
columns = columns
# Combine binary and continuous variables into a data frame
df <- data.frame(binary_indicators, continuous_variables)
# Combine binary and continuous variables into a data frame
df <- data.frame(binary_indicators, continuous_variables)
columns <- colnames(df)[1:25]
columns
# Combine binary and continuous variables into a data frame
df <- data.frame(binary_indicators, continuous_variables)
columns <- colnames(df)
columns
columns = columns
# Helper function to fit Lasso
fit_lasso <- function(features, outcome, family) {
model <- cv.glmnet(as.matrix(features), outcome, family = family, alpha = 1)
return(model)
}
# Helper function to check if a column is binary
is_binary_column <- function(column) {
unique_values <- unique(column)
return(length(unique_values) == 2 && all(unique_values %in% c(0, 1)))
}
non_binary_columns <- columns[!sapply(data[columns], is_binary_column)]
non_binary_columns
# Create basis functions for non-binary columns
basis_functions <- create_basis_functions(data, non_binary_columns, num_knots)
num_knots = 0
# Create basis functions for non-binary columns
basis_functions <- create_basis_functions(data, non_binary_columns, num_knots)
View(basis_functions)
basis_function_names <- colnames(basis_functions)
basis_function_names
basis_function_names[[1]]
load_all()
load_all()
load_all()
styler:::style_active_file()
load_all()
rm(list = c("LogicHAL"))
load_all()
# Create basis functions for non-binary columns
basis_functions <- create_basis_functions(data, non_binary_columns, num_knots)
basis_function_names <- colnames(basis_functions)
basis_function_names
View(basis_function_names)
non_binary_columns
num_knots
num_knots <- 50
# Create basis functions for non-binary columns
basis_functions <- create_basis_functions(data, non_binary_columns, num_knots)
basis_function_names <- colnames(basis_functions)
basis_function_names
load_all()
# Create basis functions for non-binary columns
basis_functions <- create_basis_functions(data, non_binary_columns, num_knots)
# Create basis functions for non-binary columns
basis_functions <- create_basis_functions(data, non_binary_columns, num_knots)
basis_function_names <- colnames(basis_functions)
basis_function_names
# Create basis functions for non-binary columns
basis_functions <- create_basis_functions(data, non_binary_columns, num_knots)
basis_function_names <- colnames(basis_functions)
# Replace non-binary columns in the original data
data <- cbind(data, basis_functions)
columns <- c(setdiff(columns, non_binary_columns), basis_function_names)
columns
View(columns)
View(data)
columns
# Build next best tree
best_info_env <- new.env()
best_info_env$best_scores <- rep(0, max_trees)
best_info_env$best_rule_paths <- list()
best_info_env$best_complexities <- rep(Inf, max_trees)
result <- build_logic_tree(data = data, outcome = outcome, columns = columns, max_depth = max_depth, best_info_env = best_info_env, max_trees = max_trees)
result
trees <- result$best_rule_path
scores <- result$best_scores
trees
scores
load_all()
library(devtools)
load_all()
# Number of observations
n <- 2000
# Create binary indicators
binary_indicators <- matrix(rbinom(n * 20, 1, 0.5), n, 20)
colnames(binary_indicators) <- paste0("bin", 1:20)
# Create continuous variables
continuous_variables <- matrix(runif(n * 5, min = 0, max = 100), n, 5)
colnames(continuous_variables) <- paste0("cont", 1:5)
# Combine binary and continuous variables into a data frame
df <- data.frame(binary_indicators, continuous_variables)
columns <- colnames(df)
# Create a continuous outcome based on a logic statement with added noise
# Example logic: outcome is high if bin1 == 1 and cont1 < 50
df$outcome <- 30 * (df$bin1 == 1 & df$cont1 < 50) + rnorm(n, mean = 0, sd = 1)
# Run LogicHAL
result <- LogicHAL(data = df, outcome = "outcome", columns = columns, max_trees = 2, max_depth = 3, family = "gaussian")
# Print final model and trees used
coef(result$model)
# Load necessary libraries
library(dplyr)
library(glmnet)
library(Rcpp)
library(LogicHAL)
load_all()
document()
install()
# Load the dataframe
file_path <- "//Users/davidmccoy/Downloads/Nura_database_merged_fingerprints.csv"
df <- read.csv(file_path, stringsAsFactors = FALSE)
# Replace 'n.a.' with NA in the AGO_PR column
df$AGO_PR[df$AGO_PR == "n.a."] <- NA
# Remove rows where AGO_PR is NA
df <- df[!is.na(df$AGO_PR), ]
# Replace 'inact.' with 0, 'act.' with 1, 'w.act.' with 1, and 'inc.' with 0 in the AGO_PR column
df$AGO_PR <- recode(df$AGO_PR, `inact.` = 0, `act.` = 1, `w.act.` = 1, `inc.` = 0)
# Ensure the AGO_PR column is numeric
df$AGO_PR <- as.numeric(df$AGO_PR)
# Identify the KRFP columns
krfp_columns <- grep("^KRFP", colnames(df), value = TRUE)
min_n_indicators <- 20
# Remove KRFP columns that are all 0 or all 1
filtered_krfp_columns <- krfp_columns[sapply(df[krfp_columns], function(x) length(unique(x)) > min_n_indicators)]
filtered_krfp_columns
min_n_indicator <- 20
# Filter KRFP columns based on the minimum number of indicators
filtered_krfp_columns <- krfp_columns[sapply(df[krfp_columns], function(x) sum(x) >= min_n_indicator)]
filtered_krfp_columns
length(filtered_krfp_columns)
# Create the filtered dataframe
filtered_data <- df %>%
select(-one_of(krfp_columns)) %>%
bind_cols(df[filtered_krfp_columns])
# Run LogicHAL on the data
result <- LogicHAL(data = filtered_data, outcome = "AGO_PR", columns = filtered_krfp_columns, max_trees = 5, max_depth = 3, family = "binomial")
