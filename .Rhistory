for (i in seq_along(iteration_results)) {
temperature_window <- c(temperature_window, rep(i, length(iteration_results[[i]]$best_scores)))
best_scores <- c(best_scores, iteration_results[[i]]$best_scores)
best_rule_paths <- c(best_rule_paths, iteration_results[[i]]$best_rule_paths)
temperatures <- c(temperatures, iteration_results[[i]]$temperature)
}
# Create the data frame
results_df <- data.frame(
temperature_window = temperature_window,
best_scores = best_scores,
best_rule_paths = best_rule_paths,
temperatures = temperatures
)
results_df
View(results_df)
results <- append(results, iteration_results)
# Convert logic paths to binary feature matrix
path_features <- sapply(results_df$best_rule_paths, function(path) as.numeric(eval(parse(text = path), envir = data)))
path_features <- as.matrix(path_features)
colnames(path_features) <- results_df$best_rule_paths
# Fit Lasso with all combined features
lasso_model <- cv.glmnet(path_features, data[[outcome]], family = family)  # Use binomial for binary outcome
# Get the coefficients from the Lasso model
lasso_coefs <- coef(lasso_model, s = "lambda.1se")
# Convert the sparse matrix to a regular matrix to work with it
coefs_matrix <- as.matrix(lasso_coefs)
# Extract the names of non-zero coefficients
nonzero_coefs <- rownames(coefs_matrix)[coefs_matrix != 0]
# If you also want to get the values of these non-zero coefficients
nonzero_values <- coefs_matrix[coefs_matrix != 0]
# Check if non-zero coefficients are stable
if (!is.null(stable_nonzero_coefs) && identical(sort(nonzero_coefs), sort(stable_nonzero_coefs))) {
stability_counter <- stability_counter + 1
if (stability_counter >= lasso_stability_threshold) {
message("Early stopping: Non-zero coefficients in Lasso model are stable.")
break
}
} else {
stable_nonzero_coefs <- nonzero_coefs
stability_counter <- 0
}
stability_counter
stable_nonzero_coefs
# Iterate through the temperature windows and decide swaps
for (i in 1:(nrow(results_df) - 1)) {
# Get current and next windows
current_window <- results_df$temperature_window[i]
next_window <- results_df$temperature_window[i + 1]
if (current_window == next_window - 1) {
# Calculate swap probability
swap_probability <- calculate_swap_probability(
results_df$best_scores[i],
results_df$best_scores[i + 1],
results_df$temperatures[i],
results_df$temperatures[i + 1]
)
# Decide swap
if (runif(1) < swap_probability) {
# Store swap in exchange_rules list
exchange_rules[[next_window]] <- results_df$best_rule_paths[i]
exchange_rules[[current_window]] <- results_df$best_rule_paths[i + 1]
} else {
# If no swap, retain original assignments
exchange_rules[[current_window]] <- ""
exchange_rules[[next_window]] <- ""
}
}
}
iteration_results
best_info_env_list <- iteration_results
exchange_rules
previous_rule_name = exchange_rules[[core_id]]
core_id
core_id <- 1
previous_rule_name = exchange_rules[[core_id]]
previous_rule_name
best_info_env = best_info_env_list[[core_id]]
temperatures = temperature_schedule_list[[core_id]][[exchange_iteration]]
temp_index_env <- new.env()
temp_index_env$current_index <- 1  # Initialize the index
no_improvement_counter <- 0
previous_best_info <- list(scores = best_info_env$best_scores, paths = best_info_env$best_rule_paths)
max_iterations <- length(temperatures)
is.null(previous_rule_name)
previous_rule <- data %>%
mutate(rule = eval(parse(text = previous_rule_name))) %>%
pull(rule)
library(dplyr)
previous_rule <- data %>%
mutate(rule = eval(parse(text = previous_rule_name))) %>%
pull(rule)
previous_rule
parent_rule_names <- list()
parent_rules <- list()
parent_scores <- list()
parent_rule_names[[1]] <- previous_rule_name
parent_rule_names
parent_rules[[1]] <- previous_rule
source("~/LogicHAL/R/utlis.R", echo=TRUE)
#'   \deqn{F1 = \frac{2 \times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}}
#'   where precision is the ratio of true positives to the total number of predicted positives, and recall is the ratio of true positives to the actual number of positives.
#'
#' @examples
#' # Example usage
#' predictions <- c(1, 0, 1, 1, 0)
#' actual <- c(1, 0, 0, 1, 1)
#' calculate_f1_score(predictions, actual)
#'
#' @export
calculate_f1_score <- function(predictions, actual) {
# Calculate True Positives, False Positives, False Negatives
tp <- sum(predictions == 1 & actual == 1)
fp <- sum(predictions == 1 & actual == 0)
fn <- sum(predictions == 0 & actual == 1)
# Calculate Precision and Recall
precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
# Calculate F1 Score
f1_score <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
return(f1_score)
}
calculate_f1_score(previous_rule, data[[outcome]])
previous_rule
previous_rule_name
parent_scores[[1]] <- calculate_f1_score(previous_rule, data[[outcome]])
parent_scores
load_all()
library(devtools)
load_all()
rm(list = c("build_logic_trees_in_parallel", "calculate_swap_probability", "computeScores", "count_logical_operators",
"create_subtree", "evaluate_all_rules", "fit_lasso", "update_best_info"))
load_all()
document()
result <- build_logic_trees_in_parallel(data, outcome, columns, max_operators, max_trees,
max_temperature, min_temperature, max_iterations,
no_improvement_threshold, num_cores, two_way_logic_roots, beam_width, n_exchange, lasso_stability_threshold)
max_temperature
max_iterations
max_iterations <- 600
result <- build_logic_trees_in_parallel(data, outcome, columns, max_operators, max_trees,
max_temperature, min_temperature, max_iterations,
no_improvement_threshold, num_cores, two_way_logic_roots, beam_width, n_exchange, lasso_stability_threshold)
#' @param min_temperature Numeric. The minimum temperature in the middle of the iterations.
#' @param max_iterations Integer. The maximum number of iterations to run the logic tree building process.
#'
#' @return List. A list containing the best scores and best rule paths found during the iteration.
#'
#' @examples
#' data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), y = sample(0:1, 100, replace = TRUE))
#' run_single_iteration(data, outcome = "y", columns = c("x1", "x2"), max_operators = 3, initial_temperature = 1, min_temperature = 0.01, max_iterations = 100)
#'
#' @export
run_single_iteration <- function(data, outcome, columns, max_operators, max_trees, temperatures, two_way_logic_roots, beam_width, no_improvement_threshold, previous_rule_name, best_info_env) {
temp_index_env <- new.env()
temp_index_env$current_index <- 1  # Initialize the index
no_improvement_counter <- 0
previous_best_info <- list(scores = best_info_env$best_scores, paths = best_info_env$best_rule_paths)
max_iterations <- length(temperatures)
if (is.null(previous_rule_name)) {
parent_rule_names <- list()
parent_rules <- list()
parent_scores <- list()
} else{
parent_rule_names <- list()
parent_rules <- list()
parent_scores <- list()
parent_rule_names[[1]] <- previous_rule_name
previous_rule <- data %>%
mutate(rule = eval(parse(text = previous_rule_name))) %>%
pull(rule)
parent_scores[[1]] <- calculate_f1_score(previous_rule, data[[outcome]])
parent_rules[[1]] <- previous_rule
}
# Flag to check if first iteration with previous rule
first_iteration_with_previous <- TRUE
while (temp_index_env$current_index < max_iterations && no_improvement_counter < no_improvement_threshold) {
iteration_time <- system.time({
result <- recursive_build_logic_tree(data, outcome, columns, max_operators,
current_operators = 1,
parent_scores = parent_scores,
parent_rules = parent_rules,
parent_rule_names = parent_rule_names,
best_info_env = best_info_env,
temperatures = temperatures,
temp_index_env = temp_index_env,
two_way_logic_roots,
beam_width, max_trees)
})
# Check for improvement
if (!identical(previous_best_info$scores, best_info_env$best_scores)) {
no_improvement_counter <- 0
previous_best_info$scores <- best_info_env$best_scores
# If improvement is made, continue using the previous rule
first_iteration_with_previous <- FALSE
} else {
no_improvement_counter <- no_improvement_counter + 1
# Reset to logic roots if no improvement and it's not the first iteration with the previous rule
if (first_iteration_with_previous) {
previous_rule_name <- ""
previous_rule <- NULL
first_iteration_with_previous <- FALSE
}
}
}
return(best_info_env)
}
run_single_iteration(data,
outcome,
columns,
max_operators,
max_trees,
temperatures = temperature_schedule_list[[core_id]][[exchange_iteration]],
two_way_logic_roots,
beam_width,
no_improvement_threshold,
previous_rule_name = exchange_rules[[core_id]],
best_info_env = best_info_env_list[[core_id]]
)
for (exchange_iteration in 1:n_exchange) {
iteration_results <- parLapply(cl, 1:num_cores, function(core_id) {
run_single_iteration(data,
outcome,
columns,
max_operators,
max_trees,
temperatures = temperature_schedule_list[[core_id]][[exchange_iteration]],
two_way_logic_roots,
beam_width,
no_improvement_threshold,
previous_rule_name = exchange_rules[[core_id]],
best_info_env = best_info_env_list[[core_id]]
)
})
# Initialize lists to store data
temperature_window <- c()
best_scores <- c()
best_rule_paths <- c()
temperatures <- c()
# Loop through the list and extract the necessary information
for (i in seq_along(iteration_results)) {
temperature_window <- c(temperature_window, rep(i, length(iteration_results[[i]]$best_scores)))
best_scores <- c(best_scores, iteration_results[[i]]$best_scores)
best_rule_paths <- c(best_rule_paths, iteration_results[[i]]$best_rule_paths)
temperatures <- c(temperatures, iteration_results[[i]]$temperature)
}
# Create the data frame
results_df <- data.frame(
temperature_window = temperature_window,
best_scores = best_scores,
best_rule_paths = best_rule_paths,
temperatures = temperatures
)
results <- append(results, iteration_results)
# Convert logic paths to binary feature matrix
path_features <- sapply(results_df$best_rule_paths, function(path) as.numeric(eval(parse(text = path), envir = data)))
path_features <- as.matrix(path_features)
colnames(path_features) <- results_df$best_rule_paths
# Fit Lasso with all combined features
lasso_model <- cv.glmnet(path_features, data[[outcome]], family = family)  # Use binomial for binary outcome
# Get the coefficients from the Lasso model
lasso_coefs <- coef(lasso_model, s = "lambda.1se")
# Convert the sparse matrix to a regular matrix to work with it
coefs_matrix <- as.matrix(lasso_coefs)
# Extract the names of non-zero coefficients
nonzero_coefs <- rownames(coefs_matrix)[coefs_matrix != 0]
# If you also want to get the values of these non-zero coefficients
nonzero_values <- coefs_matrix[coefs_matrix != 0]
# Check if non-zero coefficients are stable
if (!is.null(stable_nonzero_coefs) && identical(sort(nonzero_coefs), sort(stable_nonzero_coefs))) {
stability_counter <- stability_counter + 1
if (stability_counter >= lasso_stability_threshold) {
message("Early stopping: Non-zero coefficients in Lasso model are stable.")
break
}
} else {
stable_nonzero_coefs <- nonzero_coefs
stability_counter <- 0
}
# Iterate through the temperature windows and decide swaps
for (i in 1:(nrow(results_df) - 1)) {
# Get current and next windows
current_window <- results_df$temperature_window[i]
next_window <- results_df$temperature_window[i + 1]
if (current_window == next_window - 1) {
# Calculate swap probability
swap_probability <- calculate_swap_probability(
results_df$best_scores[i],
results_df$best_scores[i + 1],
results_df$temperatures[i],
results_df$temperatures[i + 1]
)
# Decide swap
if (runif(1) < swap_probability) {
# Store swap in exchange_rules list
exchange_rules[[next_window]] <- results_df$best_rule_paths[i]
exchange_rules[[current_window]] <- results_df$best_rule_paths[i + 1]
} else {
# If no swap, retain original assignments
exchange_rules[[current_window]] <- ""
exchange_rules[[next_window]] <- ""
}
}
}
best_info_env_list <- iteration_results
}
stopCluster(cl)
# Combine results from all parallel processes
combined_results <- data.frame(
score = numeric(),
rule_path = I(list()),
complexity = integer()
)
rule_set <- list()
for (result in results) {
for (i in 1:max_trees) {
new_rule_path <- result$best_rule_paths[[i]]
new_score <- result$best_scores[i]
new_complexity <- count_logical_operators(new_rule_path)
# Check if the rule is already in the combined results
rule_already_exists <- any(sapply(rule_set, function(rule) identical(rule, new_rule_path)))
if (!rule_already_exists) {
new_row <- data.frame(
score = new_score,
rule_path = I(list(new_rule_path)),
complexity = new_complexity
)
combined_results <- rbind(combined_results, new_row)
rule_set <- c(rule_set, list(new_rule_path))
}
}
}
# Sort the combined results by score (and complexity in case of ties)
combined_results <- combined_results[order(-combined_results$score, combined_results$complexity), ]
combined_results
load_all()
load_all()
rm(list = c("calculate_f1_score", "run_single_iteration"))
load_all()
install()
library(LogicHAL)
library(LogicHAL)
# Generate example data
set.seed(123)
# Number of observations
n <- 2000
# Number of features
p <- 100
# Create a binary matrix with some correlation
X <- matrix(rbinom(n * p, 1, 0.1), n, p)
# Create column names
colnames(X) <- paste0("var", 1:p)
# Create a binary outcome based on a rule (var1 OR var8)
Y <- as.numeric((X[, 1] | X[, 8]) & (X[, 34]))
# Combine into a data frame
df <- data.frame(X, outcome = Y)
# Run LogicHAL
result <- LogicHAL(data = df, outcome = "outcome", columns = colnames(X), max_trees = 2, max_operators = 3, family = "binomial", max_iterations = 600, no_improvement_threshold = 2, beam_width = 2, num_cores = 6, lasso_stability_threshold = 2)
load_all()
library(devtools)
load_all()
install()
library(LogicHAL)
library(LogicHAL)
# Generate example data
set.seed(123)
# Number of observations
n <- 2000
# Number of features
p <- 100
# Create a binary matrix with some correlation
X <- matrix(rbinom(n * p, 1, 0.1), n, p)
# Create column names
colnames(X) <- paste0("var", 1:p)
# Create a binary outcome based on a rule (var1 OR var8)
Y <- as.numeric((X[, 1] | X[, 8]) & (X[, 34]))
# Combine into a data frame
df <- data.frame(X, outcome = Y)
# Run LogicHAL
result <- LogicHAL(data = df, outcome = "outcome", columns = colnames(X), max_trees = 2, max_operators = 3, family = "binomial", max_iterations = 600, no_improvement_threshold = 2, beam_width = 2, num_cores = 6, lasso_stability_threshold = 2)
# Print final model and trees used
coef(result$model)
# Load necessary libraries
library(dplyr)
library(glmnet)
library(Rcpp)
library(LogicHAL)
library(randomForest)
library(caret)
library(LogicReg)
library(LogicDT)
set.seed(42)
# Load the dataframe
file_path <- "//Users/davidmccoy/Downloads/Nura_database_merged_fingerprints.csv"
df <- read.csv(file_path, stringsAsFactors = FALSE)
# Replace 'n.a.' with NA in all relevant columns
outcome_columns <- c("ANT_PR", "BIN_PR", "AGO_PXR", "ANT_PXR", "BIN_PXR", "AGO_RXR", "ANT_RXR", "BIN_RXR",
"AGO_GR", "ANT_GR", "BIN_GR", "AGO_AR", "ANT_AR", "BIN_AR", "AGO_ERA", "ANT_ERA",
"BIN_ERA", "AGO_ERB", "ANT_ERB", "BIN_ERB", "AGO_FXR", "ANT_FXR", "BIN_FXR", "AGO_PPARD",
"ANT_PPARD", "BIN_PPARD", "AGO_PPARG", "ANT_PPARG", "BIN_PPARG", "AGO_PPARA", "ANT_PPARA", "BIN_PPARA")
df[outcome_columns] <- lapply(df[outcome_columns], function(x) replace(x, x == "n.a.", NA))
# Initialize an empty list to store results
results <- list()
use_not_columns <- TRUE
# Replace 'n.a.' with NA in all relevant columns
outcome_columns <- c("ANT_PR", "BIN_PR", "AGO_PXR", "ANT_PXR", "BIN_PXR", "AGO_RXR", "ANT_RXR", "BIN_RXR",
"AGO_GR", "ANT_GR", "BIN_GR", "AGO_AR", "ANT_AR", "BIN_AR", "AGO_ERA", "ANT_ERA",
"BIN_ERA", "AGO_ERB", "ANT_ERB", "BIN_ERB", "AGO_FXR", "ANT_FXR", "BIN_FXR", "AGO_PPARD",
"ANT_PPARD", "BIN_PPARD", "AGO_PPARG", "ANT_PPARG", "BIN_PPARG", "AGO_PPARA", "ANT_PPARA", "BIN_PPARA")
df[outcome_columns] <- lapply(df[outcome_columns], function(x) replace(x, x == "n.a.", NA))
# Initialize an empty list to store results
results <- list()
use_not_columns <- TRUE
# Function to process each outcome
process_outcome <- function(outcome) {
# Remove rows where outcome is NA
data <- df[!is.na(df[[outcome]]), ]
# Replace specific values in the outcome column
data[[outcome]] <- recode(data[[outcome]], `inact.` = 0, `act.` = 1, `w.act.` = 1, `inc.` = 0)
# Ensure the outcome column is numeric
data[[outcome]] <- as.numeric(data[[outcome]])
# Identify the KRFP columns
krfp_columns <- grep("^KRFP", colnames(data), value = TRUE)
# Filter KRFP columns based on the minimum number of indicators
min_n_indicator <- 100
filtered_krfp_columns <- krfp_columns[sapply(data[krfp_columns], function(x) sum(x) >= min_n_indicator)]
# Create the filtered dataframe
filtered_data <- data %>%
select(-one_of(krfp_columns)) %>%
bind_cols(data[filtered_krfp_columns])
# Split the data into training and testing sets
train_index <- createDataPartition(filtered_data[[outcome]], p = 0.7, list = FALSE)
train_data <- filtered_data[train_index, ]
test_data <- filtered_data[-train_index, ]
# Run LogicHAL on the training data
logic_result <- LogicHAL(data = train_data, outcome = outcome, columns = filtered_krfp_columns,
max_trees = 100,
max_operators = 4, family = "binomial", num_cores = 8,
max_iterations = 1000,
no_improvement_threshold = 4, beam_width = 3,
use_not_columns = use_not_columns,
lasso_stability_threshold = 3)
# Logistic regression with logic regression
myanneal <- logreg.anneal.control(start = -1, end = -4, iter = 500, update = 100)
logreg_results <- logreg(resp = train_data[[outcome]] , bin = train_data[filtered_krfp_columns], select = 2, nleaves = 4, type = 3, ntree = 5, anneal.control = myanneal)
logreg_predictions_test <- predict(logreg_results, newbin = test_data[, c(filtered_krfp_columns)])
logreg_predictions_train <- predict(logreg_results, newbin = train_data[, c(filtered_krfp_columns)])
logreg_predictions_test <- ifelse(logreg_predictions_test > 0.5, 1, 0)
logreg_predictions_train <- ifelse(logreg_predictions_train > 0.5, 1, 0)
logreg_conf_matrix <- confusionMatrix(as.factor(logreg_predictions_test), as.factor(test_data[[outcome]]))
logreg_accuracy <- logreg_conf_matrix$overall['Accuracy']
# Train a Random Forest model on the training data
rf_model <- randomForest(as.factor(get(outcome)) ~ ., data = train_data[, c(filtered_krfp_columns, outcome)], importance = TRUE)
# Predict outcomes on the test set using Random Forest
rf_predictions <- predict(rf_model, newdata = test_data[, c(filtered_krfp_columns, outcome)])
# Evaluate Random Forest model performance
rf_conf_matrix <- confusionMatrix(as.factor(rf_predictions), as.factor(test_data[[outcome]]))
rf_accuracy <- rf_conf_matrix$overall['Accuracy']
# Evaluate LogicHAL rules on the test set
evaluate_logic_rules <- function(rules, data) {
features <- sapply(rules, function(rule) {
as.numeric(eval(parse(text = rule), envir = data))
})
features <- as.matrix(features)
colnames(features) <- rules
return(features)
}
if (use_not_columns == TRUE) {
not_data_test <- 1 - test_data[filtered_krfp_columns]
not_data_train <- 1 - train_data[filtered_krfp_columns]
# Rename columns to indicate "Not"
colnames(not_data_test) <- paste0("Not_", filtered_krfp_columns)
colnames(not_data_train) <- paste0("Not_", filtered_krfp_columns)
# Combine original and "Not" columns
test_data <- cbind(test_data, not_data_test)
train_data <- cbind(train_data, not_data_train)
}
logic_features_test <- evaluate_logic_rules(logic_result$trees, test_data)
logic_features_train <- evaluate_logic_rules(logic_result$trees, train_data)
# Predict outcomes on the test set using LogicHAL
logic_predictions_test <- predict(logic_result$model, logic_features_test, type = "response", s="lambda.1se")
logic_predictions_train <- predict(logic_result$model, logic_features_train, type = "response", s="lambda.1se")
logic_predictions <- ifelse(logic_predictions_test > 0.5, 1, 0)
# Evaluate LogicHAL model performance
logic_conf_matrix <- confusionMatrix(as.factor(as.vector(logic_predictions)), as.factor(test_data[[outcome]]))
logic_accuracy <- logic_conf_matrix$overall['Accuracy']
logic_dt_model <- logicDT(X = train_data[, filtered_krfp_columns], y = train_data[[outcome]], max_vars = 10, max_conj = 5)
logic_dt_preds <- predict(logic_dt_model, X = test_data[, filtered_krfp_columns])
logic_dt_preds <- ifelse(logic_dt_preds > 0.5, 1, 0)
logic_dt_conf_matrix <- confusionMatrix(as.factor(logic_dt_preds), as.factor(test_data[[outcome]]))
logic_dt_accuracy <- logic_dt_conf_matrix$overall['Accuracy']
# Return the results
list(
outcome = outcome,
rf_accuracy = rf_accuracy,
logic_accuracy = logic_accuracy,
logreg_accuracy = logreg_accuracy,
logic_rules = logic_result$trees
)
}
# Loop through each outcome and process
for (outcome in outcome_columns) {
tryCatch({
result <- process_outcome(outcome)
results[[outcome]] <- result
}, error = function(e) {
message(sprintf("Error processing outcome %s: %s", outcome, e$message))
})
}
result[[1]]
results
result$model
coef(result$model)
